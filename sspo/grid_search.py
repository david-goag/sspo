import os
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import pickle
import optuna

# from sspo.data import ACCESS_TOKEN, SEGMENT_LIST, collect_data
# from sspo.model import save_xgb_reg, load_xgb_reg
# from stravalib import Client as StravaClient
from sspo.registry.load_model import load_xgb_reg
from sspo.registry.upload_model import save_xgb_reg
from google.cloud import storage


def optimize_model(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.DataFrame,
    y_test: pd.DataFrame,
) -> tuple[XGBRegressor, float]:
    print("üöÄ Starting hyperparameter optimization with Optuna...")
    study = optuna.create_study(direction="minimize")

    def objective(trial: optuna.trial._trial.Trial) -> float:
        print(f"  - Running Trial #{trial.number}... ", end="")
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 700, 2200),
            "max_depth": trial.suggest_int("max_depth", 5, 15),
            "max_leaves": trial.suggest_int("max_leaves", 0, 5),
            "grow_policy": trial.suggest_categorical(
                "grow_policy", ["depthwise", "lossguide"]
            ),
            "learning_rate": trial.suggest_float(
                "learning_rate", 0.005, 0.01, log=True
            ),
            "n_jobs": -1,
            "subsample": trial.suggest_categorical(
                "subsample", [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
            ),
            "colsample_bylevel": trial.suggest_categorical(
                "colsample_bylevel", [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
            ),
            "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 100, log=True),
            "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 100, log=True),
            "seed": 42,
        }
        xgb_reg = XGBRegressor(**params, eval_metric=["rmse"])
        xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)
        rmse = round(
            mean_squared_error(y_test.values, xgb_reg.predict(X_test)) ** 0.5, 2
        )
        print(f"RMSE: {rmse}")

        return rmse

    study.optimize(objective, n_trials=100)
    print("‚úÖ Optimization complete.")
    print(f"üèÜ Best Trial #{study.best_trial.number} achieved RMSE: {study.best_value}")
    print("üìä Best Parameters Found:")
    for key, value in study.best_params.items():
        print(f"  - {key}: {value}")

    print("\n‚è≥ Retraining final model with best parameters...")
    new_xgb_reg = XGBRegressor(**study.best_params, eval_metric=["rmse"], seed=42)
    new_xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)
    new_rmse = round(
        mean_squared_error(y_test.values, new_xgb_reg.predict(X_test)) ** 0.5, 2
    )
    print(f"‚úÖ Final model retrained. Validation RMSE: {new_rmse}")

    return (new_xgb_reg, new_rmse)


def get_best_model_filename() -> str:
    print("\nüîç Searching for the current best model...")
    BUCKET_NAME = os.environ["BUCKET_NAME"]
    client = storage.Client()
    bucket = client.bucket(BUCKET_NAME)
    blobs = bucket.list_blobs(max_results=1000)
    directory = []
    for blob in blobs:
        directory.append(blob.name)
    xgb_filename = None
    metric = 1000
    for file in directory:
        if file.startswith("models/xgb_reg_"):
            splitted = file.split("_")
            new_metric = int(splitted[2]) + int(splitted[3].split(".")[0]) / 100
            if new_metric < metric:
                xgb_filename = f"xgb_reg_{splitted[2]}_{splitted[3].split('.')[0]}"
                metric = new_metric

    if xgb_filename:
        print(f"‚ú® Found best current model: '{xgb_filename}' with RMSE: {metric}")
    else:
        print("‚ö†Ô∏è No existing model found.")

    return xgb_filename


def retrain_model() -> None:
    # NEED TO BE REPLACED WITH THE CLOUD DATABASE
    print("\n--- Starting Model Retraining and Evaluation ---")
    file_path = Path("database/test_fit/all_athletes_20250903175112.parquet")
    df = pd.read_parquet(file_path)
    print(f"‚úÖ Data loaded successfully from '{file_path}'. Shape: {df.shape}")
    #df.set_index("id", inplace=True)
    df = df.drop(columns=["power_max"])
    X = df.drop(columns=["time"])
    y = df.time
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(
        f"‚úÖ Data split into training ({X_train.shape[0]} rows) and testing ({X_test.shape[0]} rows) sets."
    )

    new_xgb_reg, new_rmse = optimize_model(X_train, X_test, y_train, y_test)

    best_model_filename = get_best_model_filename()
    if best_model_filename:
        current_xgb_reg = load_xgb_reg(best_model_filename)
        current_xgb_reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)
        current_rmse = round(
            mean_squared_error(y_test.values, current_xgb_reg.predict(X_test)) ** 0.5, 2
        )
        print(
            f"\nüìà Current Model RMSE: {current_rmse} vs. New Optimized Model RMSE: {new_rmse}"
        )
        if new_rmse < current_rmse:
            print(f"üéâ New model is better! Saving new model...")
            save_xgb_reg(new_xgb_reg, f"xgb_reg_{str(new_rmse).replace('.', '_')}")
        else:
            print("üëç Current model is still the best. No new model will be saved.")
    else:
        print(
            "\n‚úÖ No existing model to compare against. Saving the first optimized model."
        )
        save_xgb_reg(new_xgb_reg, f"xgb_reg_{str(new_rmse).replace('.', '_')}")


if __name__ == "__main__":
    retrain_model()
    print("\n--- Process Complete ---")
